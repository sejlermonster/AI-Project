\chapter{Implementation}
\label{chp:impl}

This chapter describes the implementation of the project. The project tries to solve the kidnapped robot problem. This is a classic problem where a self driving robot is carried to an arbitrary location in a world. In this project the robot must find its way to a predefined location as well. In the implementation a Particle filter is used for localization while A* is used for path finding.

\section{Hardware}
The chosen hardware for the project is a Lego Mindstorm EV3 robot. The used robot includes two motors along with a gyroscope sensor for angle measurement and an ultrasound sensor for distance measurement. This is sufficient for solving the kidnapped robot problem. The assembled robot is shown in figure \ref{fig:legoRobot}

\myFigure{LegoRobot.jpg}{Assembled Lego Mindstorm EV3 robot}{fig:legoRobot}{0.7} 
\FloatBarrier

Using the motors, gyroscope and ultrasound sensor showed some inaccuracy.  The gyroscope has a documented accuracy of +/- 3 degrees and the ultrasonic sensor has a documented accuracy of +/-1 centimeters. Further experiments showed that the ultrasonic sensor could not guarantee any accuracy if the angle to the measured object was not perpendicular. This gave some problems throughout the development and the ultrasonic sensor could yield measurements of for example 90 centimeters when the distance was only 10 centimeters. 

Through the various test runs the robot also showed different performance depending on the current battery life. This meant that to get a proper test run, it was important to make sure that the battery was fully charged. 

Even though the Lego Mindstorm robot was very inaccurate in some situations it allowed for focus on developing software instead of setting up hardware and sensors.

\subsection{Devloping for Lego Mindstorm}
A framework(https://github.com/BrianPeek/legoev3/wiki) was choosen to develop for the Lego Mindstorm. This framework allowed to code in C\# and communicate with the Lego Mindstorm robot via Bluetooth from a computer. This enabled for easily developing advanced algorithms for the Lego Mindstorm robot. By using Bluetooth communication between the robot and a PC it was also easy to follow all of the algorithms performed.

\section{Structure and design}
The structure of the developed software is illustrated with a UML class diagram in figure \ref{fig:class}. Note that the diagram only illustrates public functions, this was chosen to make it easier to read. The class \emph{Main} initializes the world map and the landmarks in it. \emph{Main} also initializes \emph{ParticleFilter}, \emph{Astar}, \emph{RobotMotion} and \emph{RobotSensing}. The \emph{ParticleFilter} class manipulates a list of the type \emph{Particle}. The class \emph{Particle} uses \emph{GaussianNoise} to apply gaussian noise for movement and measurements. \emph{RobotMotion} and \emph{RobotSensing} is used to control and sense with the robot.

\myFigure{classDiagram.png}{UML class diagram of the implemented project}{fig:class}{1} 

In the following sections implementation details a further explained.

\section{Robot motion and measurements}
\subsection{Motion}
Robot motion is implemented for rotation around the center of the robot and movement forwards and backwards. The rotation of the robot is implemented with a \emph{point turn}, where one wheel moves forward while the other moves backwards. The rotation of the robot is illustrated on figure \ref{fig:rotation}


\myFigure{RobotRotation.png}{Robot rotation around point \citep{pointTurn}}{fig:rotation}{0.7} 

The movement forwards and backwards simply applies force to both motors. 

On both rotation and movement a P controller was implemented. When rotating the robot, the gyroscope was used for calculating the error. For movements, the ultrasonic sensor was used for calculating the error. In general for both rotation and movement a very small force was applied to the robot and a small threshold was set as acceptable for the movements and rotation. This was to ensure that the robot did not keep oscillating.

Listing \ref{lst:turn} is pseudo code for the implementation of turning the robot. It shows that the endDegree is calculated based on the startDegree plus the angle that the robot should turn. A while loop then keeps running until it hits the threshold. If it overshoots the robot turns one way and if it undershoots the robot turns the other way.

\lstset{style=sharpc}
\begin{lstlisting}[caption={Pseudo code for turning the robot}, label=lst:turn, mathescape=true]             
var startDegree = _robotSensing.GetGyroData();
var endDegree = startDegree + degrees;
while (currentDegree < endDegree - treshhold 
|| currentDegree > endDegree + treshhold)
{	
	.....
}
\end{lstlisting}


\FloatBarrier
\subsection{Measurements}
The robot had both gyroscope and an ultrasonic sensor which in the project was used for measuring angle and distance. They were both used for implementing more precise movements while the ultrasonic sensor was also used for the particle filter. A scan method was therefore implemented to scan the area. The scan function is illustrated in figure \ref{fig:scan}. The robot rotates around itself while taking measurements. Four measurements was taking during a scan at every 90 degrees.

\myFigure{scan.png}{Robot scan}{fig:scan}{0.5} 

\FloatBarrier
\section{Particle Filter}
In this project a particle filter was implemented and used for localization of the robot. 

The particle filter is initialized with 3 parameters: the number of particles to be randomly generated and the length on the x and y axis.

\lstset{style=sharpc}
\begin{lstlisting}[caption={Initialization of the particle filter}, label=lst:initPart, mathescape=true]             
_particleFilter.GenerateParticles(_numberOfParticles, world.GetLength(0),
world.GetLength(1));
\end{lstlisting}

After the particles are generated the Particle filter can be used for moving around the particles and resampling. The resampling is implemented as explained in chapter \ref{chp:partFilter} with the resampling wheel algorithm. The implemented resampling function is shown in Listing \ref{lst:wheel}. The method is called with a measurement. First a method is called for calculating the importance weight of each particle. This function looks at each particle and calculates the distance to the nearest object in the orientation of the particle. This distance is then compared with the measurement and based on this the weight is calculated.

When the importance weight is calculated a new list of particles is created \emph{particles2}. The resampling wheel algorithms then adds particles to the new list. Based on the importance weight, the particle has a higher probability of being added to the new list of particles. When the algorithm is done the list of particles is set equal to the new list \emph{particles2} and the weights are normalized.

\lstset{style=sharpc}
\begin{lstlisting}[caption={Resampling wheel}, label=lst:wheel, mathescape=true]                          
public void Resampling(double measurement)
{
	SetImportanceWeight(measurement);
	var particles2 = new List<Particle>();
	var index = random.Next(0, Particles.Count);
	var beta = 0.0;
	var maxParticleWeight = Particles.Max(x => x.Weight);
	var max = maxParticleWeight;
	for (int i = 0; i < Particles.Count; i++)
	{
		beta = beta + random.NextDouble() * 2*max;
		while (beta > Particles[index].Weight)
		{
			beta -= Particles[index].Weight;
			index = (index + 1) % Particles.Count;
		}
		particles2.Add(Particles[index].Clone());
	}
	Particles = particles2;
	NormalizeParticleWeights();
}
\end{lstlisting}

When moving particles forwards or backwards the function MoveParticle is used. This function can be seen in listing \ref{lst:move}. The function ensures that the particles are moved in the direction of the particle by using sine and cosine. When the new position is set for a particle, Gaussian noise is added for the particle. This is done because the movement of the robot is inaccurate. 

\lstset{style=sharpc}
\begin{lstlisting}[caption={Resampling wheel}, label=lst:move,  basicstyle=\tiny, mathescape=true]        
public void MoveParticles(double distance)
 {
 	foreach (var particle in Particles)
 	{
 		particle.Position = new Point(
 		particle.Position.X + Math.Cos(particle.OrientationInRadians) * distance, 
 		particle.Position.Y + Math.Sin(particle.OrientationInRadians) * distance); 
 		particle.ApplyGaussianNoiseForPosition();
 	}
 }
\end{lstlisting}

\subsection{Simulation}
This section presents a simple simulation of the particle filter. In the example the simulated robot is placed in the world at coordinates (23,64) and will first scan the area, then drive forward twice and scan the area again. When the simulated robot drives forward it also measures the distance when it's finished driving. This is similar to how the Lego Mindstorm robot is implemented.

In figure \ref{fig:sim1} the world is initialized. The red dot is the robot, the direction of the robot is displayed by the small line drawn from the square. The particles are the black dots. The landmarks placed in the world are colored green, orange and blue. In the illustration the particles are spread out uniformly in the world. In figure \ref{fig:sim2} the robot has made its first 360 degree scan and taken a measurement every 90 degrees. The particles are now centered around a few specific places in the world. The location is still very uncertain, this is mainly caused by the symmetry of the world.

\mySubFigure{Simulation/1_Initialized.PNG}{Simulation/2_afterScan.PNG}{}{Intialized particles and world}{Robot makes a 360 degree scan}{fig:sim}{fig:sim1}{fig:sim2}

In figure \ref{fig:sim3} and \ref{fig:sim4} the robot drives forward 10 centimeters twice. The robot takes a measurement after each 10 centimeters. As illustrated the particles are now beginning to be concentrated around the simulated robot.

\mySubFigure{Simulation/3_afterMoveAndMeas.PNG}{Simulation/4_afterMoveAndMeas.PNG}{}{Robot moves forward and measures distance}{Robot moves forward again and measures distance}{fig:simA}{fig:sim3}{fig:sim4}

\FloatBarrier
In figure \ref{fig:5} the robots makes 360 degree scan and takes measurement every 90 degree. After the scan the location is very certain and the particles are all located around the simulated robot.

\myFigure{Simulation/5_afterScan.PNG}{Robot makes a 360 degree scan}{fig:5}{0.5} 

\section{A*}
When the approximate location of the robot in this project is found, an A* algorithm is used to determine a path to the goal destination. The A* algorithm is initialized with 3 parameters: the length on the x an y axis, and the coordinates of the landmarks of the environment in which the robot is placed. 

\lstset{style=sharpc}
\begin{lstlisting}[caption={Initialization of the A* algorithm}, label=lst:Astar_init, mathescape=true]             
_aStarSearch = new AStarSearch(world.GetLength(0), world.GetLength(1),
 _landmarks);
\end{lstlisting}

It uses the environment information to create its heuristic function and to make a path from its own position to the goal destination. The basis for the algorithm is explained in chapter \ref{chp:search} and as mentioned the A* algorithm is a more efficient variant of the Search algorithm, meaning the basic functionality is the same. 

When the algorithm has knowledge of the environment, it can begin searching for paths to the goal destination. It does this by exploring adjacent paths to see if they are walkable. However it first checks two things: if it has run out of paths to explore, or if it is already at the goal destination. If neither of these are true it will go into the for-loop illustrated in listing \ref{lst:search_func}. 

\lstset{style=sharpc}
\begin{lstlisting}[caption={Functionality of Search}, label=lst:search_func, mathescape=true]             
for (int i = 0; i < delta.Count; i++)
{
	int x2 = x + delta[i][0];
	int y2 = y + delta[i][1];

	if (x2 >= 0 && x2 < _worldMatrixWithPossiblePaths.GetLength(0) && 
	    y2 >= 0 && y2 < _worldMatrixWithPossiblePaths.GetLength(1))
	{
		if(worldClosed[x2,y2] == 0 && 
		   _worldMatrixWithPossiblePaths[x2,y2] == 0 || 
		   worldClosed[x2, y2] == 99 && 
		   _worldMatrixWithPossiblePaths[x2, y2] == 0)
		{
			int g2 = g + cost;
			var h2 = heuristic[x2, y2];
			var f2 = g2 + h2;
			open.Add(new List<int>()
			{
				f2, g2, h2, x2, y2
			});
			worldClosed[x2, y2] = 2;
			possibleMovements.Add(new List<int>()
			{
				f2, x2, y2
			});
		}
	}
}
\end{lstlisting}

\emph{delta} is a list of directions the robot can go to explore new paths, meaning the loop checks all the directions from the state at that moment. For every one of those directions it checks if it can move that way, or if there is an obstacle. It also checks if the direction is even inside the environment in which it is placed. If the direction is indeed a possible path to expand to it will add it to the list of open expansions, it will close the path so it cannot be explored more than once, and it will add it to the list of possible movements.

The A* part of this loop is when the heuristic function is introduced. \emph{g2} is the distance value from the robots position to the path point in question, \emph{h2} is the distance from the path point to the goal, and \emph{f2} is the combination of the two. By combining them, the algorithm can chose the shortest path to the goal.

\subsection{Simulation}
In this section a simple simulation of the A* algorithm is illustrated. A 10x10 array is set up with some obstacles. The algorithm is given its start position and its goal destination. All this is illustrated in the first part of the console output shown in figure \ref{fig:consOut}, where:

\begin{itemize}
	\item \emph{S} is the start location
	\item \emph{E} is the goal destination
	\item \emph{*} is the free space
	\item The rest are the obstacles
\end{itemize}

The algorithm then produces the heuristic function, and outputs it. This is shown in the second part of the console output. Since the heuristic function alone does not take obstacles into account, it is represented by a 10x10 array filled with distance values to the goal. 

When the algorithm has found its optimal path to the goal it outputs an array containing the distance from start to goal, along with the x an y-value of the goal destination. This is the third section of the console output, meaning that for this example the distance from start to goal is 12 steps. 

The optimal path that was found by the algorithm is shown in the last part of the console output, and shows that the robot will not need to do any unnecessary movement to get to the goal destination. The reason it does not just show one path, is that there are more paths with equal cost. Note that the printed arrows indicates how the robot has moved to reach that specific position and the algorithm successfully found an optimal path.

\myFigure{Simulation/AstarSimulate.JPG}{A* simulation console output}{fig:consOut}{0.2}

\section{Combining Particle Filter and A*}

The Particle filter and the A* algorithm handles two different aspects of the kidnapped robot problem. The Particle filter makes sure the robot can find its own position in an arbitrary environment, and the A* algorithm makes sure the robot can find a path to its final predefined destination. This showed a strong combination for solving the problem.

\myFigure{iterationPartA.PNG}{Iterates between localization and path finding}{fig:itePartA}{0.8}
\FloatBarrier
When placing the robot in the world it starts by scanning the area and updating the particle filter. A position and orientation in the map is estimated by calculating the mean of the particles location and orientation. This location and orientation is the expected and is used as start position for the A* algorithm. The robot then moves 10 centimeters along the calculated path. After moving it scans again and resamples the particles. This iteration keeps going until the robot reaches it target location. This iteration between localization and path finding is illustrated in figure \ref{fig:itePartA}. 


